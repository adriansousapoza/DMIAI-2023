{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, batch 0/7, loss: 0.709789514541626\n",
      "Epoch 0, batch 1/7, loss: 0.7127237319946289\n",
      "Epoch 0, batch 2/7, loss: 0.6989465355873108\n",
      "Epoch 0, batch 3/7, loss: 0.6916332840919495\n",
      "Epoch 0, batch 4/7, loss: 0.6905733942985535\n",
      "Epoch 0, batch 5/7, loss: 0.6598642468452454\n",
      "Epoch 0, batch 6/7, loss: 0.7205790877342224\n",
      "Epoch 0 loss: 4.884109795093536\n",
      "Epoch 1, batch 0/7, loss: 0.6408108472824097\n",
      "Epoch 1, batch 1/7, loss: 0.6342605352401733\n",
      "Epoch 1, batch 2/7, loss: 0.6334906816482544\n",
      "Epoch 1, batch 3/7, loss: 0.5846535563468933\n",
      "Epoch 1, batch 4/7, loss: 0.5929581522941589\n",
      "Epoch 1, batch 5/7, loss: 0.5860046148300171\n",
      "Epoch 1, batch 6/7, loss: 0.5745774507522583\n",
      "Epoch 1 loss: 4.246755838394165\n",
      "Epoch 2, batch 0/7, loss: 0.5046137571334839\n",
      "Epoch 2, batch 1/7, loss: 0.4994351267814636\n",
      "Epoch 2, batch 2/7, loss: 0.5142229795455933\n",
      "Epoch 2, batch 3/7, loss: 0.5040744543075562\n",
      "Epoch 2, batch 4/7, loss: 0.4356571435928345\n",
      "Epoch 2, batch 5/7, loss: 0.35930025577545166\n",
      "Epoch 2, batch 6/7, loss: 0.29508399963378906\n",
      "Epoch 2 loss: 3.112387716770172\n",
      "Epoch 3, batch 0/7, loss: 0.29620829224586487\n",
      "Epoch 3, batch 1/7, loss: 0.2780168354511261\n",
      "Epoch 3, batch 2/7, loss: 0.3319133520126343\n",
      "Epoch 3, batch 3/7, loss: 0.2580599784851074\n",
      "Epoch 3, batch 4/7, loss: 0.16266800463199615\n",
      "Epoch 3, batch 5/7, loss: 0.1876194030046463\n",
      "Epoch 3, batch 6/7, loss: 0.24254032969474792\n",
      "Epoch 3 loss: 1.757026195526123\n",
      "Epoch 4, batch 0/7, loss: 0.15384340286254883\n",
      "Epoch 4, batch 1/7, loss: 0.18412919342517853\n",
      "Epoch 4, batch 2/7, loss: 0.09577105939388275\n",
      "Epoch 4, batch 3/7, loss: 0.07901734113693237\n",
      "Epoch 4, batch 4/7, loss: 0.18945908546447754\n",
      "Epoch 4, batch 5/7, loss: 0.08364702761173248\n",
      "Epoch 4, batch 6/7, loss: 0.06492393463850021\n",
      "Epoch 4 loss: 0.8507910445332527\n",
      "Epoch 5, batch 0/7, loss: 0.06354160606861115\n",
      "Epoch 5, batch 1/7, loss: 0.054748110473155975\n",
      "Epoch 5, batch 2/7, loss: 0.05190592259168625\n",
      "Epoch 5, batch 3/7, loss: 0.04738934338092804\n",
      "Epoch 5, batch 4/7, loss: 0.03506430983543396\n",
      "Epoch 5, batch 5/7, loss: 0.10272315889596939\n",
      "Epoch 5, batch 6/7, loss: 0.024197690188884735\n",
      "Epoch 5 loss: 0.3795701414346695\n",
      "Epoch 6, batch 0/7, loss: 0.0317898690700531\n",
      "Epoch 6, batch 1/7, loss: 0.048671986907720566\n",
      "Epoch 6, batch 2/7, loss: 0.03757431358098984\n",
      "Epoch 6, batch 3/7, loss: 0.021124834194779396\n",
      "Epoch 6, batch 4/7, loss: 0.021238431334495544\n",
      "Epoch 6, batch 5/7, loss: 0.020534541457891464\n",
      "Epoch 6, batch 6/7, loss: 0.023659495636820793\n",
      "Epoch 6 loss: 0.2045934721827507\n",
      "Epoch 7, batch 0/7, loss: 0.022041523829102516\n",
      "Epoch 7, batch 1/7, loss: 0.015172373503446579\n",
      "Epoch 7, batch 2/7, loss: 0.016148025169968605\n",
      "Epoch 7, batch 3/7, loss: 0.01597338542342186\n",
      "Epoch 7, batch 4/7, loss: 0.013465492986142635\n",
      "Epoch 7, batch 5/7, loss: 0.012341705150902271\n",
      "Epoch 7, batch 6/7, loss: 0.01187337376177311\n",
      "Epoch 7 loss: 0.10701587982475758\n",
      "Epoch 8, batch 0/7, loss: 0.0141909783706069\n",
      "Epoch 8, batch 1/7, loss: 0.011007636785507202\n",
      "Epoch 8, batch 2/7, loss: 0.011854272335767746\n",
      "Epoch 8, batch 3/7, loss: 0.010809422470629215\n",
      "Epoch 8, batch 4/7, loss: 0.010275520384311676\n",
      "Epoch 8, batch 5/7, loss: 0.010842600837349892\n",
      "Epoch 8, batch 6/7, loss: 0.009123902767896652\n",
      "Epoch 8 loss: 0.07810433395206928\n",
      "Epoch 9, batch 0/7, loss: 0.009262477047741413\n",
      "Epoch 9, batch 1/7, loss: 0.009793409146368504\n",
      "Epoch 9, batch 2/7, loss: 0.008285689167678356\n",
      "Epoch 9, batch 3/7, loss: 0.008606460876762867\n",
      "Epoch 9, batch 4/7, loss: 0.008182494901120663\n",
      "Epoch 9, batch 5/7, loss: 0.00817262101918459\n",
      "Epoch 9, batch 6/7, loss: 0.007684378884732723\n",
      "Epoch 9 loss: 0.059987531043589115\n",
      "Epoch 10, batch 0/7, loss: 0.0075205471366643906\n",
      "Epoch 10, batch 1/7, loss: 0.008082826621830463\n",
      "Epoch 10, batch 2/7, loss: 0.006893536541610956\n",
      "Epoch 10, batch 3/7, loss: 0.007241273298859596\n",
      "Epoch 10, batch 4/7, loss: 0.00708415824919939\n",
      "Epoch 10, batch 5/7, loss: 0.006564778741449118\n",
      "Epoch 10, batch 6/7, loss: 0.0057640960440039635\n",
      "Epoch 10 loss: 0.04915121663361788\n",
      "Epoch 11, batch 0/7, loss: 0.006292320787906647\n",
      "Epoch 11, batch 1/7, loss: 0.006250472739338875\n",
      "Epoch 11, batch 2/7, loss: 0.005865774117410183\n",
      "Epoch 11, batch 3/7, loss: 0.0058734179474413395\n",
      "Epoch 11, batch 4/7, loss: 0.00621459586545825\n",
      "Epoch 11, batch 5/7, loss: 0.006214393302798271\n",
      "Epoch 11, batch 6/7, loss: 0.005784482695162296\n",
      "Epoch 11 loss: 0.04249545745551586\n",
      "Epoch 12, batch 0/7, loss: 0.005267021246254444\n",
      "Epoch 12, batch 1/7, loss: 0.005195689853280783\n",
      "Epoch 12, batch 2/7, loss: 0.005593069363385439\n",
      "Epoch 12, batch 3/7, loss: 0.005676581524312496\n",
      "Epoch 12, batch 4/7, loss: 0.00557705108076334\n",
      "Epoch 12, batch 5/7, loss: 0.005227523855865002\n",
      "Epoch 12, batch 6/7, loss: 0.004579312168061733\n",
      "Epoch 12 loss: 0.03711624909192324\n",
      "Epoch 13, batch 0/7, loss: 0.0051689958199858665\n",
      "Epoch 13, batch 1/7, loss: 0.005347074009478092\n",
      "Epoch 13, batch 2/7, loss: 0.004982843063771725\n",
      "Epoch 13, batch 3/7, loss: 0.005266339983791113\n",
      "Epoch 13, batch 4/7, loss: 0.004914979450404644\n",
      "Epoch 13, batch 5/7, loss: 0.004475112073123455\n",
      "Epoch 13, batch 6/7, loss: 0.00453032273799181\n",
      "Epoch 13 loss: 0.034685667138546705\n",
      "Epoch 14, batch 0/7, loss: 0.0051424382254481316\n",
      "Epoch 14, batch 1/7, loss: 0.0044298660941421986\n",
      "Epoch 14, batch 2/7, loss: 0.0045795440673828125\n",
      "Epoch 14, batch 3/7, loss: 0.004662563093006611\n",
      "Epoch 14, batch 4/7, loss: 0.004425046965479851\n",
      "Epoch 14, batch 5/7, loss: 0.0044510383158922195\n",
      "Epoch 14, batch 6/7, loss: 0.004284488037228584\n",
      "Epoch 14 loss: 0.03197498479858041\n",
      "Epoch 15, batch 0/7, loss: 0.004215096589177847\n",
      "Epoch 15, batch 1/7, loss: 0.0039467578753829\n",
      "Epoch 15, batch 2/7, loss: 0.004374524112790823\n",
      "Epoch 15, batch 3/7, loss: 0.004013712052255869\n",
      "Epoch 15, batch 4/7, loss: 0.0039321561343967915\n",
      "Epoch 15, batch 5/7, loss: 0.004149904008954763\n",
      "Epoch 15, batch 6/7, loss: 0.0033430696930736303\n",
      "Epoch 15 loss: 0.027975220466032624\n",
      "Epoch 16, batch 0/7, loss: 0.003678784007206559\n",
      "Epoch 16, batch 1/7, loss: 0.0035785785876214504\n",
      "Epoch 16, batch 2/7, loss: 0.0037176422774791718\n",
      "Epoch 16, batch 3/7, loss: 0.004059426486492157\n",
      "Epoch 16, batch 4/7, loss: 0.0036924113519489765\n",
      "Epoch 16, batch 5/7, loss: 0.0035340464673936367\n",
      "Epoch 16, batch 6/7, loss: 0.003695462131872773\n",
      "Epoch 16 loss: 0.025956351310014725\n",
      "Epoch 17, batch 0/7, loss: 0.003622658783569932\n",
      "Epoch 17, batch 1/7, loss: 0.0036529996432363987\n",
      "Epoch 17, batch 2/7, loss: 0.0035484754480421543\n",
      "Epoch 17, batch 3/7, loss: 0.0033467307221144438\n",
      "Epoch 17, batch 4/7, loss: 0.003793703392148018\n",
      "Epoch 17, batch 5/7, loss: 0.0030767458956688643\n",
      "Epoch 17, batch 6/7, loss: 0.0037975849118083715\n",
      "Epoch 17 loss: 0.024838898796588182\n",
      "Epoch 18, batch 0/7, loss: 0.003573131514713168\n",
      "Epoch 18, batch 1/7, loss: 0.003042717929929495\n",
      "Epoch 18, batch 2/7, loss: 0.0032115697395056486\n",
      "Epoch 18, batch 3/7, loss: 0.003254040377214551\n",
      "Epoch 18, batch 4/7, loss: 0.0032782042399048805\n",
      "Epoch 18, batch 5/7, loss: 0.0034897292498499155\n",
      "Epoch 18, batch 6/7, loss: 0.0033767065033316612\n",
      "Epoch 18 loss: 0.02322609955444932\n",
      "Epoch 19, batch 0/7, loss: 0.003282989142462611\n",
      "Epoch 19, batch 1/7, loss: 0.0029453965835273266\n",
      "Epoch 19, batch 2/7, loss: 0.0030865652952343225\n",
      "Epoch 19, batch 3/7, loss: 0.003096077824011445\n",
      "Epoch 19, batch 4/7, loss: 0.0029200874269008636\n",
      "Epoch 19, batch 5/7, loss: 0.0030507133342325687\n",
      "Epoch 19, batch 6/7, loss: 0.0028884734492748976\n",
      "Epoch 19 loss: 0.021270303055644035\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "model_name = \"bert-base-multilingual-cased\"  # or another model suitable for Danish\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# load texts and labels\n",
    "def load_data():\n",
    "\n",
    "    import os\n",
    "\n",
    "    data = {\n",
    "        'human': [],\n",
    "        'bot': []\n",
    "    }\n",
    "\n",
    "    for filename in os.listdir('data/heste-nettet-nyheder'):\n",
    "        with open('data/heste-nettet-nyheder/' + filename, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "            text = '\\n'.join(content.split('\\n')[2:])\n",
    "            data['human'].append(text)\n",
    "\n",
    "    for filename in os.listdir('data/heste-nettet-nyheder-ai/gpt-3.5-turbo/'):\n",
    "        with open('data/heste-nettet-nyheder-ai/gpt-3.5-turbo/' + filename, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "            text = '\\n'.join(content.split('\\n')[2:])\n",
    "            data['bot'].append(text)\n",
    "\n",
    "    my_texts = np.array(data['human'] + data['bot'])\n",
    "    my_labels = np.array([0]*len(data['human']) + [1]*len(data['bot'])) \n",
    "    \n",
    "\n",
    "    return list(my_texts), my_labels\n",
    "\n",
    "def preprocess(text):\n",
    "    inputs = tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    return inputs\n",
    "\n",
    "texts, labels = load_data()\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.inputs = preprocess(texts)\n",
    "\n",
    "        # dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
    "        self.input_ids = self.inputs['input_ids']\n",
    "        self.attention_mask = self.inputs['attention_mask']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        input_ids = self.input_ids[idx]\n",
    "        attention_mask = self.attention_mask[idx]\n",
    "\n",
    "        return {\n",
    "            # 'text': text,\n",
    "            'label': label,\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask\n",
    "        }\n",
    "\n",
    "\n",
    "your_texts, your_labels = load_data()\n",
    "dataset = TextDataset(your_texts, your_labels)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Assume you're using a GPU for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    loss_total = 0\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        #rint(outputs)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_total += loss.item()\n",
    "        print(f\"Epoch {epoch}, batch {i}/{len(dataloader)}, loss: {loss.item()}\")\n",
    "\n",
    "    print(f\"Epoch {epoch} loss: {loss_total}\")\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"models/bert_classifier\", save_function=torch.save)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load my model\n",
    "from transformers import BertForSequenceClassification\n",
    "model = BertForSequenceClassification.from_pretrained(\"models/bert_classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4077, 0.2017]])\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "\n",
    "test_string = \"This is a test\"\n",
    "\n",
    "model.eval()    \n",
    "with torch.no_grad():\n",
    "    inputs = preprocess(test_string)\n",
    "    input_ids = inputs['input_ids'].squeeze(1).to(device)\n",
    "    attention_mask = inputs['attention_mask'].squeeze(1).to(device)\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "    print(logits)\n",
    "    print(torch.argmax(logits).item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "t2mENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
