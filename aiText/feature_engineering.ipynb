{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Simon Andersen\\Projects\\Projects\\DMIAI_2023\\aiText\\feature_engineering.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Simon%20Andersen/Projects/Projects/DMIAI_2023/aiText/feature_engineering.ipynb#W0sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdecomposition\u001b[39;00m \u001b[39mimport\u001b[39;00m LatentDirichletAllocation\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Simon%20Andersen/Projects/Projects/DMIAI_2023/aiText/feature_engineering.ipynb#W0sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m make_multilabel_classification\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Simon%20Andersen/Projects/Projects/DMIAI_2023/aiText/feature_engineering.ipynb#W0sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m pipeline\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Simon%20Andersen/Projects/Projects/DMIAI_2023/aiText/feature_engineering.ipynb#W0sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_extraction\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtext\u001b[39;00m \u001b[39mimport\u001b[39;00m TfidfVectorizer\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Simon%20Andersen/Projects/Projects/DMIAI_2023/aiText/feature_engineering.ipynb#W0sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_extraction\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtext\u001b[39;00m \u001b[39mimport\u001b[39;00m CountVectorizer\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from transformers import pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import  CountVectorizer\n",
    "\n",
    "#!pip install  wordcloud\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "\n",
    "nltk.download('punkt');\n",
    "nltk.download('wordnet');\n",
    "nltk.download('stopwords');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_data(df, inplace = True):\n",
    "    \"\"\"\n",
    "    For each row in the dataframe, find the number of sentences,\n",
    "    and the average and std number of words per sentence.\n",
    "    \"\"\"\n",
    "\n",
    "    # Make a copy of the dataframe\n",
    "    df = df.copy()\n",
    "\n",
    "    # Get the number of sentences\n",
    "    df['num_sentences'] = df['text'].apply(lambda x: len(x.split('.')))\n",
    "\n",
    "    # Get the average number of words per sentence\n",
    "    df['avg_words_per_sentence'] = df['text'].apply(lambda x: np.mean([len(sentence.split(' ')) for sentence in x.split('.')]))\n",
    "\n",
    "    # Get the std number of words per sentence\n",
    "    df['std_words_per_sentence'] = df['text'].apply(lambda x: np.std([len(sentence.split(' ')) for sentence in x.split('.')]))\n",
    "\n",
    "    # Get the average number of words per sentence\n",
    "    df['avg_words_per_sentence'] = df['text'].apply(lambda x: np.mean([len(sentence.split(' ')) for sentence in x.split('.')]))\n",
    "\n",
    "    # Get the std number of words per sentence\n",
    "    df['std_words_per_sentence'] = df['text'].apply(lambda x: np.std([len(sentence.split(' ')) for sentence in x.split('.')]))\n",
    "\n",
    "    if inplace:\n",
    "        return df\n",
    "    else:\n",
    "        return df[['num_sentences', 'avg_words_per_sentence', 'std_words_per_sentence']]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def analyze_sentences(df_text):\n",
    "    \"\"\"\n",
    "    For each row in the dataframe, find the number of sentences,\n",
    "    the average and std number of words per sentence, the average and std number of words between sentences,\n",
    "    and the avg. no. of abbreviations and capital letters per sentence.\n",
    "\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_text: pd.DataFrame\n",
    "        Dataframe containing the text column\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sentence_count, mean_words_per_sentence, std_words_per_sentence, mean_diff_words, std_diff_words, avg_abbrev_per_sentence, \\\n",
    "        avg_capital_letters_per_sentence, avg_dash_quest_semicolon_per_sentence, double_quest_excl_count\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    sentences = nltk.tokenize.sent_tokenize(df_text)\n",
    "    sentence_count = len(sentences)\n",
    "\n",
    "\n",
    "    if sentence_count == 0:\n",
    "        return 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
    "    \n",
    "    words_per_sentence = np.array([len(nltk.tokenize.word_tokenize(sentence)) for sentence in sentences])\n",
    "\n",
    "    total_abbrev_count = 0\n",
    "    total_capital_count = 0\n",
    "    total_dash_quest_semicolon_count = 0\n",
    "    total_double_quest_excl_count = 0\n",
    "    total_excl_count = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Count abbreviations using a simple regex pattern\n",
    "        abbrev_count = len(re.findall(r'\\b[A-Za-z]{2,}\\.(?![a-z])', sentence))\n",
    "        total_abbrev_count += abbrev_count\n",
    "        \n",
    "        # Count capital letters\n",
    "        capital_count = sum(1 for c in sentence if c.isupper())\n",
    "        total_capital_count += capital_count\n",
    "\n",
    "        # Count dashes, question marks, and semicolons\n",
    "        dash_quest_semicolon_count = sentence.count('-') + sentence.count('?') + sentence.count(';')\n",
    "        total_dash_quest_semicolon_count += dash_quest_semicolon_count\n",
    "\n",
    "        # Count double question marks and exclamation marks\n",
    "        double_quest_excl_count = sentence.count('??') + sentence.count('!!') + sentence.count('?!') + sentence.count('!?')\n",
    "        total_double_quest_excl_count += double_quest_excl_count\n",
    "\n",
    "        # Count exclamation marks\n",
    "        excl_count = sentence.count('!')\n",
    "        total_excl_count += excl_count\n",
    "\n",
    "    # Exclude ! from the count if it ends the last sentence or follows hej or spørgsmål\n",
    "    last_sentence = sentences[-1]\n",
    "    total_excl_count -= last_sentence.count('!')\n",
    "    total_excl_count -= len(re.findall(r'\\b(?:hej|spørgsmål)!', sentences[0]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if sentence_count > 1:\n",
    "        # find mean and std for the length of consecutive sentences\n",
    "        diff_words_between_sentences = np.diff(words_per_sentence)\n",
    "        mean_diff_words = np.mean(diff_words_between_sentences)\n",
    "        std_diff_words = np.std(diff_words_between_sentences)\n",
    "    else:\n",
    "        mean_diff_words = 0\n",
    "        std_diff_words = 0\n",
    "    \n",
    "\n",
    "    avg_abbrev_per_sentence = total_abbrev_count / sentence_count\n",
    "    avg_capital_letters_per_sentence = total_capital_count / sentence_count\n",
    "    avg_dash_quest_semicolon_per_sentence = total_dash_quest_semicolon_count / sentence_count\n",
    "    avg_excl_per_sentence = total_excl_count / sentence_count\n",
    " \n",
    "    mean_words_per_sentence = np.mean(words_per_sentence)\n",
    "    std_words_per_sentence = np.std(words_per_sentence)\n",
    "\n",
    "    \n",
    "\n",
    "    return sentence_count, mean_words_per_sentence, std_words_per_sentence, mean_diff_words, std_diff_words, avg_abbrev_per_sentence, \\\n",
    "        avg_capital_letters_per_sentence, avg_dash_quest_semicolon_per_sentence, total_double_quest_excl_count, avg_excl_per_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_specific_words(text, word_lists):\n",
    "    sentences = nltk.tokenize.sent_tokenize(text)\n",
    "    sentence_count = len(sentences)\n",
    "    \n",
    "    # Initialize a list to store counts for each word list separately\n",
    "    word_counts_per_list = [[] for _ in range(len(word_lists))]\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        for idx, word_list in enumerate(word_lists):\n",
    "            # Initialize a counter for specified words in the current word list\n",
    "            word_count = 0\n",
    "            \n",
    "            # Explicit variations of words in each word list to count\n",
    "            for word in word_list:\n",
    "                word_count += sentence.count(f'{word}')  # Space before and after\n",
    "                word_count += sentence.count(f'{word.upper()}')\n",
    "                # Add other variations as needed\n",
    "            \n",
    "            # Append the word count for the current word list\n",
    "            word_counts_per_list[idx].append(word_count)\n",
    "    \n",
    "    # Calculate the average count per sentence for each word list\n",
    "    avg_words_per_sentence = [sum(counts) / sentence_count if sentence_count > 0 else 0 for counts in word_counts_per_list]\n",
    "    \n",
    "    # return each value separately\n",
    "    \n",
    "    return avg_words_per_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of conversations:  1082\n",
      "Columns in conversations.csv:  ['text']\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('val.csv')\n",
    "\n",
    "N = data.shape[0]\n",
    "\n",
    "print('Number of conversations: ', N)\n",
    "print(\"Columns in conversations.csv: \", list(data.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['sentence_count', 'mean_words_per_sentence', 'std_words_per_sentence', 'neighbor_sentences_diff', \\\n",
    "      'neighbor_sentences_diff_std', 'abbrev_per_sentence', 'captials_per_sentence', 'avg_dash_quest_semicolon_per_sentence', 'double_quest_excl_count', 'avg_excl_per_sentence']] = data['text'].apply(analyze_sentences).apply(pd.Series)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plural_pronouns = [' vi ', ' os ', ' vores ']\n",
    "\n",
    "human_fillers = [' jo ', ' jo,' ' jo!', ' lige ', ' sådan noget',  ' bestemt ',  ' bestemt!',  ' bestemt,',  ' bestemt.', ' gerne ', ' gerne!', ' gerne,', ' gerne.', ' rigtig god']\n",
    "ai_fillers = [' samt ', ' dette ', ' mens ', ' dog ']\n",
    "ai_fillers2 = [' en vis ', ' sammenfattende ']\n",
    "woke_list = [' tilbøjelig ', ' tilbøjelige ', ' parter ', ' grupper ', ' organisationer ', ' organisationer.', ' føle sig ', ' føler sig ']\n",
    "fake_news_list = [' subjektiv ', ' kilder ', ' kilder.', ' studier ', ' forklaringer ',\n",
    "' manipulation ', ' disinformation ', ' disinformation.', ' misinformation ', ' misinformation.' ' da det ',]\n",
    "ai_words1 = [' vigtigt, at', ' vigtigt at ']\n",
    "ai_words2 = [' svært at ']\n",
    "\n",
    "word_lists = [human_fillers, ai_fillers, ai_fillers2, woke_list, fake_news_list, ai_words1, ai_words2]\n",
    "column_names = ['human_fillers', 'ai_fillers', 'ai_fillers2', 'woke', 'fake_news', 'ai_words1', 'ai_words2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'count_specific_words' and 'word_lists' are defined as mentioned in the previous code snippet\n",
    "\n",
    "# Apply count_specific_words along with additional arguments using lambda function\n",
    "data[[col for col in column_names]] = data.apply(lambda row: count_specific_words(row['text'], word_lists), axis=1).apply(pd.Series)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1082 entries, 0 to 1081\n",
      "Data columns (total 18 columns):\n",
      " #   Column                                 Non-Null Count  Dtype  \n",
      "---  ------                                 --------------  -----  \n",
      " 0   text                                   1082 non-null   object \n",
      " 1   sentence_count                         1082 non-null   float64\n",
      " 2   mean_words_per_sentence                1082 non-null   float64\n",
      " 3   std_words_per_sentence                 1082 non-null   float64\n",
      " 4   neighbor_sentences_diff                1082 non-null   float64\n",
      " 5   neighbor_sentences_diff_std            1082 non-null   float64\n",
      " 6   abbrev_per_sentence                    1082 non-null   float64\n",
      " 7   captials_per_sentence                  1082 non-null   float64\n",
      " 8   avg_dash_quest_semicolon_per_sentence  1082 non-null   float64\n",
      " 9   double_quest_excl_count                1082 non-null   float64\n",
      " 10  avg_excl_per_sentence                  1082 non-null   float64\n",
      " 11  human_fillers                          1082 non-null   float64\n",
      " 12  ai_fillers                             1082 non-null   float64\n",
      " 13  ai_fillers2                            1082 non-null   float64\n",
      " 14  woke                                   1082 non-null   float64\n",
      " 15  fake_news                              1082 non-null   float64\n",
      " 16  ai_words1                              1082 non-null   float64\n",
      " 17  ai_words2                              1082 non-null   float64\n",
      "dtypes: float64(17), object(1)\n",
      "memory usage: 152.3+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print first 100 text rows\n",
    "for idx, text in data.iterrows():\n",
    "    print(\"\\nNY\")\n",
    "    print(text['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate csv data file for each label\n",
    "dir_human = 'C:\\\\Users\\\\Simon Andersen\\\\Projects\\\\Projects\\\\DMIAI_2023\\\\aiText\\\\data\\\\heste-nettet-nyheder'\n",
    "dir_ai4 = 'C:\\\\Users\\\\Simon Andersen\\\\Projects\\\\Projects\\\\DMIAI_2023\\\\aiText\\\\data\\\\heste-nettet-nyheder-ai\\\\gpt-4-0613'\n",
    "dir_ai3 = 'C:\\\\Users\\\\Simon Andersen\\\\Projects\\\\Projects\\\\DMIAI_2023\\\\aiText\\\\data\\\\heste-nettet-nyheder-ai\\\\gpt-3.5-turbo'\n",
    "# write a function to find all txt files in a directory\n",
    "def find_txt_files(path):\n",
    "    txt_files = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                txt_files.append(os.path.join(root, file))\n",
    "    return txt_files\n",
    "\n",
    "# find all txt files in the directory\n",
    "path_human = find_txt_files(dir_human)\n",
    "path_ai4 = find_txt_files(dir_ai4)\n",
    "path_ai3 = find_txt_files(dir_ai3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function to open a text file return the last line as a string\n",
    "def read_human_files(paths):\n",
    "    human_files = []\n",
    "    for path in paths:\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            last_line = lines[-1]\n",
    "            human_files.append(last_line)\n",
    "    return human_files\n",
    "\n",
    "def read_files(paths, line_start = 0):\n",
    "    file_contents = []\n",
    "\n",
    "    for file_path in paths:\n",
    "        try:\n",
    "            with open(file_path, 'r') as file:\n",
    "                lines = file.readlines()\n",
    "                # Ignore the first three lines and store the rest\n",
    "                remaining_lines = ''.join(lines[line_start:])\n",
    "                file_contents.append(remaining_lines)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return file_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_human = read_human_files(path_human)\n",
    "text_ai3 = read_files(path_ai3, line_start = 3)\n",
    "text_ai4 = read_files(path_ai4, line_start = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human:  4698\n",
      "AI:  731\n"
     ]
    }
   ],
   "source": [
    "Nhuman = len(text_human)\n",
    "Nai3 = len(text_ai3)\n",
    "Nai4 = len(text_ai4)\n",
    "\n",
    "print(\"Human: \", Nhuman)\n",
    "print(\"AI: \", Nai3 + Nai4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with the text and label (human = 0 or ai = 1) for each text list\n",
    "df_human = pd.DataFrame({'text': text_human, 'label': [0]*len(text_human)})\n",
    "df_ai3 = pd.DataFrame({'text': text_ai3, 'label': [1]*len(text_ai3)})\n",
    "df_ai4 = pd.DataFrame({'text': text_ai4, 'label': [1]*len(text_ai4)})\n",
    "df= pd.concat([df_human, df_ai3, df_ai4], ignore_index=True)\n",
    "\n",
    "# shuffle the dataframe\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
